{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 06. Ensemble\n",
    "\n",
    "과제는 총 3개의 cell과 markdown 하나를 채워주셔야합니다\n",
    "\n",
    "1~3 문제는 영상과 실습자료에 나와있는 것들을 적절히 응용하시면 됩니다\n",
    "\n",
    "마지막 4번의 경우 본인의 생각을 마크다운으로 작성하는 부분입니다\n",
    "\n",
    "주석의 경우 이미지, 테이블 등의 표현이 어려운 관계로 받지 않겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 00. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 과제에 사용할 dataset은 The census - Adult dataset입니다\n",
    "\n",
    "ML dataset repository인 UCI에서 받을 수 있습니다\n",
    "\n",
    "파일의 마지막 열에는 target 정보가 기록되어있습니다\n",
    "\n",
    "데이터를 불러온 후 variables와 target으로 분리해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset  -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>77516</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>83311</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>215646</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>234721</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>338409</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1       2  3   4  5  6  7  8  9    10  11  12  13  14\n",
       "0  39  0   77516  0  13  0  0  0  0  0  2174   0  40   0   0\n",
       "1  50  1   83311  0  13  1  1  1  0  0     0   0  13   0   0\n",
       "2  38  2  215646  1   9  2  2  0  0  0     0   0  40   0   0\n",
       "3  53  2  234721  2   7  1  2  1  1  0     0   0  40   0   0\n",
       "4  28  2  338409  0  13  1  3  2  1  1     0   0  40   1   0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv('week_06_ensemble.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df.loc[:,'0':'13']\n",
    "target = df.loc[:,'14':'14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32561, 14), (32561, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터를 학습하여 보지않은 test data에 대해 잘 분류해야합니다\n",
    "\n",
    "아래 train test split 함수를 사용하여 train set과 test set으로 분리해주세요\n",
    "\n",
    "이후 모든 모델 학습의 결과는 test set으로 평가해주셔야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(variables, target, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 01. Voting ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset을 약 200개의 base learner에 학습시킨 뒤, 정확도를 출력해주세요\n",
    "\n",
    "실습한 3가지 모델을 전부 사용해주셔야 하지만, 각 모델의 hyper parameter는 자유롭게 사용해주세요\n",
    "\n",
    "Hard voting과 soft voting을 각각 시행하여 결과를 출력해주셔야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models=[('GNBC', GaussianNB())]+[(f'kNN-{n}', KNeighborsClassifier(n)) for n in (1,30)]+\\\n",
    "        [(f'DT-{_}',DecisionTreeClassifier()) for _ in range(170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    39,      0,  77516, ...,      0,     40,      0],\n",
       "       [    50,      1,  83311, ...,      0,     13,      0],\n",
       "       [    38,      2, 215646, ...,      0,     40,      0],\n",
       "       ...,\n",
       "       [    35,      2, 174308, ...,      0,     40,      0],\n",
       "       [    31,      1, 162551, ...,      0,     50,      4],\n",
       "       [    39,      6, 372525, ...,      0,     60,      0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble score : 81.492%\n",
      "GNBC's score : 79.65%\n",
      "kNN-1's score : 72.505%\n",
      "kNN-30's score : 79.374%\n",
      "DT-0's score : 81.134%\n",
      "DT-1's score : 81.175%\n",
      "DT-2's score : 81.144%\n",
      "DT-3's score : 81.114%\n",
      "DT-4's score : 81.063%\n",
      "DT-5's score : 81.349%\n",
      "DT-6's score : 81.083%\n",
      "DT-7's score : 81.001%\n",
      "DT-8's score : 81.032%\n",
      "DT-9's score : 80.929%\n",
      "DT-10's score : 80.786%\n",
      "DT-11's score : 81.165%\n",
      "DT-12's score : 81.124%\n",
      "DT-13's score : 81.226%\n",
      "DT-14's score : 81.206%\n",
      "DT-15's score : 81.022%\n",
      "DT-16's score : 81.421%\n",
      "DT-17's score : 81.298%\n",
      "DT-18's score : 81.124%\n",
      "DT-19's score : 81.298%\n",
      "DT-20's score : 81.032%\n",
      "DT-21's score : 81.083%\n",
      "DT-22's score : 81.339%\n",
      "DT-23's score : 81.114%\n",
      "DT-24's score : 81.308%\n",
      "DT-25's score : 81.298%\n",
      "DT-26's score : 80.94%\n",
      "DT-27's score : 81.011%\n",
      "DT-28's score : 80.95%\n",
      "DT-29's score : 80.94%\n",
      "DT-30's score : 81.073%\n",
      "DT-31's score : 81.144%\n",
      "DT-32's score : 81.093%\n",
      "DT-33's score : 81.022%\n",
      "DT-34's score : 81.37%\n",
      "DT-35's score : 81.114%\n",
      "DT-36's score : 81.165%\n",
      "DT-37's score : 80.96%\n",
      "DT-38's score : 81.175%\n",
      "DT-39's score : 81.063%\n",
      "DT-40's score : 81.022%\n",
      "DT-41's score : 81.278%\n",
      "DT-42's score : 80.96%\n",
      "DT-43's score : 81.083%\n",
      "DT-44's score : 81.165%\n",
      "DT-45's score : 81.278%\n",
      "DT-46's score : 81.073%\n",
      "DT-47's score : 81.318%\n",
      "DT-48's score : 81.042%\n",
      "DT-49's score : 81.196%\n",
      "DT-50's score : 81.339%\n",
      "DT-51's score : 81.247%\n",
      "DT-52's score : 81.103%\n",
      "DT-53's score : 81.103%\n",
      "DT-54's score : 80.929%\n",
      "DT-55's score : 81.063%\n",
      "DT-56's score : 81.165%\n",
      "DT-57's score : 80.571%\n",
      "DT-58's score : 81.38%\n",
      "DT-59's score : 81.196%\n",
      "DT-60's score : 80.97%\n",
      "DT-61's score : 81.257%\n",
      "DT-62's score : 81.38%\n",
      "DT-63's score : 81.278%\n",
      "DT-64's score : 80.796%\n",
      "DT-65's score : 81.257%\n",
      "DT-66's score : 81.38%\n",
      "DT-67's score : 81.144%\n",
      "DT-68's score : 81.462%\n",
      "DT-69's score : 81.359%\n",
      "DT-70's score : 81.052%\n",
      "DT-71's score : 81.196%\n",
      "DT-72's score : 81.339%\n",
      "DT-73's score : 81.144%\n",
      "DT-74's score : 81.216%\n",
      "DT-75's score : 81.124%\n",
      "DT-76's score : 81.247%\n",
      "DT-77's score : 81.083%\n",
      "DT-78's score : 81.216%\n",
      "DT-79's score : 81.001%\n",
      "DT-80's score : 81.226%\n",
      "DT-81's score : 81.052%\n",
      "DT-82's score : 81.237%\n",
      "DT-83's score : 80.817%\n",
      "DT-84's score : 81.226%\n",
      "DT-85's score : 81.196%\n",
      "DT-86's score : 81.001%\n",
      "DT-87's score : 81.001%\n",
      "DT-88's score : 80.715%\n",
      "DT-89's score : 81.237%\n",
      "DT-90's score : 80.827%\n",
      "DT-91's score : 80.991%\n",
      "DT-92's score : 81.359%\n",
      "DT-93's score : 81.247%\n",
      "DT-94's score : 81.226%\n",
      "DT-95's score : 81.134%\n",
      "DT-96's score : 80.96%\n",
      "DT-97's score : 81.155%\n",
      "DT-98's score : 81.165%\n",
      "DT-99's score : 81.257%\n",
      "DT-100's score : 80.97%\n",
      "DT-101's score : 81.144%\n",
      "DT-102's score : 81.011%\n",
      "DT-103's score : 81.196%\n",
      "DT-104's score : 81.144%\n",
      "DT-105's score : 81.052%\n",
      "DT-106's score : 81.288%\n",
      "DT-107's score : 81.114%\n",
      "DT-108's score : 81.185%\n",
      "DT-109's score : 81.185%\n",
      "DT-110's score : 81.063%\n",
      "DT-111's score : 81.144%\n",
      "DT-112's score : 80.929%\n",
      "DT-113's score : 81.267%\n",
      "DT-114's score : 81.185%\n",
      "DT-115's score : 81.052%\n",
      "DT-116's score : 81.185%\n",
      "DT-117's score : 81.298%\n",
      "DT-118's score : 81.257%\n",
      "DT-119's score : 81.032%\n",
      "DT-120's score : 81.267%\n",
      "DT-121's score : 81.39%\n",
      "DT-122's score : 81.001%\n",
      "DT-123's score : 81.339%\n",
      "DT-124's score : 81.4%\n",
      "DT-125's score : 80.909%\n",
      "DT-126's score : 81.165%\n",
      "DT-127's score : 81.329%\n",
      "DT-128's score : 80.858%\n",
      "DT-129's score : 81.103%\n",
      "DT-130's score : 80.909%\n",
      "DT-131's score : 81.206%\n",
      "DT-132's score : 80.909%\n",
      "DT-133's score : 81.278%\n",
      "DT-134's score : 81.206%\n",
      "DT-135's score : 80.97%\n",
      "DT-136's score : 81.359%\n",
      "DT-137's score : 81.216%\n",
      "DT-138's score : 81.001%\n",
      "DT-139's score : 80.919%\n",
      "DT-140's score : 81.278%\n",
      "DT-141's score : 81.042%\n",
      "DT-142's score : 81.083%\n",
      "DT-143's score : 81.288%\n",
      "DT-144's score : 80.95%\n",
      "DT-145's score : 81.462%\n",
      "DT-146's score : 81.318%\n",
      "DT-147's score : 81.523%\n",
      "DT-148's score : 80.715%\n",
      "DT-149's score : 81.206%\n",
      "DT-150's score : 81.482%\n",
      "DT-151's score : 80.95%\n",
      "DT-152's score : 81.267%\n",
      "DT-153's score : 80.929%\n",
      "DT-154's score : 81.39%\n",
      "DT-155's score : 80.981%\n",
      "DT-156's score : 81.206%\n",
      "DT-157's score : 81.349%\n",
      "DT-158's score : 81.175%\n",
      "DT-159's score : 81.032%\n",
      "DT-160's score : 81.073%\n",
      "DT-161's score : 81.359%\n",
      "DT-162's score : 81.103%\n",
      "DT-163's score : 80.858%\n",
      "DT-164's score : 80.981%\n",
      "DT-165's score : 80.96%\n",
      "DT-166's score : 81.144%\n",
      "DT-167's score : 81.237%\n",
      "DT-168's score : 81.421%\n",
      "DT-169's score : 81.257%\n"
     ]
    }
   ],
   "source": [
    "copy_models=models.copy()\n",
    "\n",
    "ensemble=VotingClassifier(models, voting='hard')\n",
    "\n",
    "ensemble.fit(x_train.values, y_train.values.ravel())\n",
    "\n",
    "print(f'ensemble score : {ensemble.score(x_test,y_test)*100:.5}%')\n",
    "\n",
    "\n",
    "for clf in copy_models:\n",
    "    clf[1].fit(x_train.values,y_train.values.ravel())\n",
    "    print(f\"{clf[0]}'s score : {clf[1].score(x_test,y_test)*100:.5}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT-147's score : 81.523%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble score : 81.278%\n",
      "GNBC's score : 79.65%\n",
      "kNN-1's score : 72.505%\n",
      "kNN-30's score : 79.374%\n",
      "DT-0's score : 81.185%\n",
      "DT-1's score : 80.899%\n",
      "DT-2's score : 81.093%\n",
      "DT-3's score : 81.278%\n",
      "DT-4's score : 81.052%\n",
      "DT-5's score : 81.237%\n",
      "DT-6's score : 81.298%\n",
      "DT-7's score : 81.134%\n",
      "DT-8's score : 81.042%\n",
      "DT-9's score : 81.226%\n",
      "DT-10's score : 81.503%\n",
      "DT-11's score : 81.073%\n",
      "DT-12's score : 81.4%\n",
      "DT-13's score : 81.032%\n",
      "DT-14's score : 80.981%\n",
      "DT-15's score : 81.339%\n",
      "DT-16's score : 81.093%\n",
      "DT-17's score : 81.237%\n",
      "DT-18's score : 81.318%\n",
      "DT-19's score : 81.144%\n",
      "DT-20's score : 81.001%\n",
      "DT-21's score : 81.38%\n",
      "DT-22's score : 81.185%\n",
      "DT-23's score : 81.103%\n",
      "DT-24's score : 81.349%\n",
      "DT-25's score : 80.97%\n",
      "DT-26's score : 81.032%\n",
      "DT-27's score : 81.001%\n",
      "DT-28's score : 81.206%\n",
      "DT-29's score : 80.981%\n",
      "DT-30's score : 81.421%\n",
      "DT-31's score : 81.237%\n",
      "DT-32's score : 81.308%\n",
      "DT-33's score : 81.175%\n",
      "DT-34's score : 80.909%\n",
      "DT-35's score : 80.929%\n",
      "DT-36's score : 81.175%\n",
      "DT-37's score : 80.909%\n",
      "DT-38's score : 81.001%\n",
      "DT-39's score : 81.032%\n",
      "DT-40's score : 81.329%\n",
      "DT-41's score : 81.165%\n",
      "DT-42's score : 81.185%\n",
      "DT-43's score : 81.452%\n",
      "DT-44's score : 80.96%\n",
      "DT-45's score : 81.237%\n",
      "DT-46's score : 81.298%\n",
      "DT-47's score : 81.196%\n",
      "DT-48's score : 81.083%\n",
      "DT-49's score : 81.206%\n",
      "DT-50's score : 80.97%\n",
      "DT-51's score : 81.083%\n",
      "DT-52's score : 81.001%\n",
      "DT-53's score : 81.237%\n",
      "DT-54's score : 81.411%\n",
      "DT-55's score : 80.97%\n",
      "DT-56's score : 80.94%\n",
      "DT-57's score : 81.308%\n",
      "DT-58's score : 81.237%\n",
      "DT-59's score : 81.267%\n",
      "DT-60's score : 81.001%\n",
      "DT-61's score : 81.237%\n",
      "DT-62's score : 80.827%\n",
      "DT-63's score : 81.093%\n",
      "DT-64's score : 81.093%\n",
      "DT-65's score : 81.022%\n",
      "DT-66's score : 81.042%\n",
      "DT-67's score : 81.206%\n",
      "DT-68's score : 80.95%\n",
      "DT-69's score : 81.175%\n",
      "DT-70's score : 81.124%\n",
      "DT-71's score : 81.093%\n",
      "DT-72's score : 80.919%\n",
      "DT-73's score : 81.155%\n",
      "DT-74's score : 81.032%\n",
      "DT-75's score : 81.155%\n",
      "DT-76's score : 81.185%\n",
      "DT-77's score : 81.185%\n",
      "DT-78's score : 81.237%\n",
      "DT-79's score : 81.001%\n",
      "DT-80's score : 81.411%\n",
      "DT-81's score : 81.052%\n",
      "DT-82's score : 81.114%\n",
      "DT-83's score : 81.359%\n",
      "DT-84's score : 80.991%\n",
      "DT-85's score : 81.216%\n",
      "DT-86's score : 81.155%\n",
      "DT-87's score : 81.298%\n",
      "DT-88's score : 81.503%\n",
      "DT-89's score : 81.308%\n",
      "DT-90's score : 81.114%\n",
      "DT-91's score : 81.278%\n",
      "DT-92's score : 81.288%\n",
      "DT-93's score : 80.735%\n",
      "DT-94's score : 81.38%\n",
      "DT-95's score : 81.042%\n",
      "DT-96's score : 81.226%\n",
      "DT-97's score : 80.837%\n",
      "DT-98's score : 81.124%\n",
      "DT-99's score : 81.134%\n",
      "DT-100's score : 81.185%\n",
      "DT-101's score : 81.114%\n",
      "DT-102's score : 81.533%\n",
      "DT-103's score : 81.267%\n",
      "DT-104's score : 81.226%\n",
      "DT-105's score : 81.175%\n",
      "DT-106's score : 81.175%\n",
      "DT-107's score : 81.165%\n",
      "DT-108's score : 81.257%\n",
      "DT-109's score : 81.022%\n",
      "DT-110's score : 81.247%\n",
      "DT-111's score : 81.114%\n",
      "DT-112's score : 80.878%\n",
      "DT-113's score : 80.848%\n",
      "DT-114's score : 81.083%\n",
      "DT-115's score : 81.022%\n",
      "DT-116's score : 81.093%\n",
      "DT-117's score : 81.155%\n",
      "DT-118's score : 80.919%\n",
      "DT-119's score : 81.114%\n",
      "DT-120's score : 81.226%\n",
      "DT-121's score : 81.083%\n",
      "DT-122's score : 81.175%\n",
      "DT-123's score : 81.063%\n",
      "DT-124's score : 81.165%\n",
      "DT-125's score : 81.656%\n",
      "DT-126's score : 81.278%\n",
      "DT-127's score : 81.226%\n",
      "DT-128's score : 81.042%\n",
      "DT-129's score : 81.042%\n",
      "DT-130's score : 80.827%\n",
      "DT-131's score : 81.144%\n",
      "DT-132's score : 81.267%\n",
      "DT-133's score : 80.796%\n",
      "DT-134's score : 81.554%\n",
      "DT-135's score : 81.083%\n",
      "DT-136's score : 80.899%\n",
      "DT-137's score : 81.513%\n",
      "DT-138's score : 80.919%\n",
      "DT-139's score : 81.237%\n",
      "DT-140's score : 81.063%\n",
      "DT-141's score : 81.052%\n",
      "DT-142's score : 81.308%\n",
      "DT-143's score : 81.278%\n",
      "DT-144's score : 81.39%\n",
      "DT-145's score : 81.103%\n",
      "DT-146's score : 81.257%\n",
      "DT-147's score : 81.103%\n",
      "DT-148's score : 81.134%\n",
      "DT-149's score : 81.216%\n",
      "DT-150's score : 81.339%\n",
      "DT-151's score : 81.318%\n",
      "DT-152's score : 80.97%\n",
      "DT-153's score : 81.022%\n",
      "DT-154's score : 81.226%\n",
      "DT-155's score : 81.196%\n",
      "DT-156's score : 81.093%\n",
      "DT-157's score : 81.39%\n",
      "DT-158's score : 81.216%\n",
      "DT-159's score : 81.247%\n",
      "DT-160's score : 81.042%\n",
      "DT-161's score : 81.114%\n",
      "DT-162's score : 81.349%\n",
      "DT-163's score : 81.124%\n",
      "DT-164's score : 81.533%\n",
      "DT-165's score : 80.878%\n",
      "DT-166's score : 81.339%\n",
      "DT-167's score : 81.441%\n",
      "DT-168's score : 80.919%\n",
      "DT-169's score : 81.308%\n"
     ]
    }
   ],
   "source": [
    "copy_models=models.copy()\n",
    "\n",
    "ensemble=VotingClassifier(models, voting='soft')\n",
    "\n",
    "ensemble.fit(x_train.values, y_train.values.ravel())\n",
    "\n",
    "print(f'ensemble score : {ensemble.score(x_test,y_test)*100:.5}%')\n",
    "\n",
    "\n",
    "for clf in copy_models:\n",
    "    clf[1].fit(x_train.values,y_train.values.ravel())\n",
    "    print(f\"{clf[0]}'s score : {clf[1].score(x_test,y_test)*100:.5}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 02. Bootstrap Aggregating (bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bagging을 실습해보겠습니다\n",
    "\n",
    "Random forest는 bagging을 지원하지만 다양한 classifier를 사용하는 경우 bagging을 직접 구현해야합니다\n",
    "\n",
    "bagging을 하기위한 package들도 존재하지만 numpy를 사용하여 bagging을 간단히 적용할 수 있습니다\n",
    "\n",
    "먼저 sample만 무작위로 복원추출 하는 함수를 작성해주세요\n",
    "\n",
    "np.random.choice 함수를 사용하여 slicing하면 간단히 구현할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_bootstrap(mat,bootstrap_ratio=0.6):\n",
    "    \n",
    "    size=len(mat)\n",
    "    num = size * bootstrap_ratio\n",
    "    \n",
    "    num_list = []\n",
    "    for i in range(int(num)):\n",
    "        num_list.append(np.random.choice(size))\n",
    "        \n",
    "    bootstrap_mat = df.iloc[num_list]\n",
    "                            \n",
    "    return bootstrap_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot=[]\n",
    "for i in range(0,5):\n",
    "    boot.append(sample_bootstrap(df, bootstrap_ratio=0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 sample과 함께 variables들도 복원추출할 수 있는 함수를 작성해주세요\n",
    "\n",
    "위와 같이 함수를 사용하여 row뿐만 아니라 column도 slicing하는 것으로 구현할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_list = []\n",
    "target_list = []\n",
    "def bootstrap(mat=[],bootstrap_ratio=0.6):\n",
    "    for i in range(len(boot)):\n",
    "        variables_list.append(mat[i].loc[:,'0':'13'])\n",
    "        target_list.append(mat[i].loc[:,'14':'14'])\n",
    "    bootstrap_mat = (variables_list,target_list)\n",
    "    return bootstrap_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([        0  1       2   3   4  5   6  7  8  9    10    11  12  13\n",
       "  30765  38  2  252897   5  10  2   5  3  0  1     0     0  25   0\n",
       "  31470  19  2  188568   5  10  0  14  0  0  1     0     0  40   0\n",
       "  11486  42  2  334522   1   9  1   2  1  1  0     0     0  40   0\n",
       "  5694   44  2  182616   7  11  1  10  1  0  0     0     0  40   0\n",
       "  16344  28  2   72443   5  10  0   1  0  0  0     0  1669  60   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..   ...   ...  ..  ..\n",
       "  11836  49  3  586657   0  13  1   1  1  0  0  7298     0  40   0\n",
       "  5712   47  0   55272   5  10  1   6  1  0  0     0     0  40   0\n",
       "  30750  25  2  122999   1   9  0   6  5  0  0     0     0  40   0\n",
       "  30147  31  2  243773   4   5  0   4  4  0  1     0     0  20   0\n",
       "  2278   17  2  329783  12   6  0   5  5  0  1     0     0  10   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9  10    11  12  13\n",
       "  21722  49  2  149809  14   1  1   4  1  0  0   0     0  40   4\n",
       "  22343  64  2  285052   5  10  6   1  0  0  1   0     0  10   0\n",
       "  1457   22  2  310152   5  10  0   7  0  0  0   0     0  40   0\n",
       "  18147  58  2  205410   1   9  1   6  1  0  0   0     0  40   0\n",
       "  19975  37  2  187748   5  10  1   6  1  0  0   0     0  40   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..  ..   ...  ..  ..\n",
       "  14954  35  2  409200   6  12  0  12  0  0  0   0     0  40   0\n",
       "  19342  26  2  266022   5  10  1   5  1  0  0   0     0  50   0\n",
       "  11229  32  2  244147   0  13  0   3  0  0  0   0  1876  50   0\n",
       "  31271  74  2   54732   5  10  1   5  1  0  0   0     0  20   0\n",
       "  31008  47  0  187087   0  13  1   1  1  0  0   0     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10  11  12  13\n",
       "  29537  46  4  133969  3  14  2   3  0  3  0      0   0  40   0\n",
       "  3593   61  1   32423  1   9  1   8  2  0  1  22040   0  40   0\n",
       "  2660   48  2  108993  1   9  1   6  1  0  0      0   0  45   0\n",
       "  32025  50  2  139703  5  10  1   6  1  0  0      0   0  40   0\n",
       "  4343   33  4   43959  3  14  1   3  1  0  0   7688   0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...  ..  ..  ..\n",
       "  20868  61  5  253101  0  13  2  11  0  0  1      0   0  24   0\n",
       "  10181  43  2  108945  5  10  0   1  0  0  1      0   0  50   0\n",
       "  18207  44  2  271792  0  13  3   1  4  0  1      0   0  40   0\n",
       "  20777  49  2  183013  1   9  1   8  1  0  0      0   0  40   0\n",
       "  30850  37  4  126569  1   9  0   6  0  0  0      0   0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10    11  12  13\n",
       "  6666   58  2   87329  2   7  1   0  1  0  0      0     0  48   0\n",
       "  6596   28  2  125527  5  10  0   5  0  0  0      0     0  50   0\n",
       "  29925  43  2  323713  1   9  1   9  1  0  0      0     0  40   0\n",
       "  18712  34  2   54850  0  13  0   1  0  0  0      0  1590  50   0\n",
       "  13786  53  2   47396  5  10  1   0  1  0  0      0     0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...   ...  ..  ..\n",
       "  28019  35  2  340428  6  12  0   0  0  0  1      0     0  35   0\n",
       "  17957  68  4  242095  0  13  1   1  1  0  0  20051     0  40   0\n",
       "  28106  22  2  221480  1   9  0   0  3  0  1      0     0  40   0\n",
       "  8949   60  5  366531  7  11  6  11  0  0  1      0     0  40   0\n",
       "  16147  41  2  152742  7  11  2  10  0  0  1   3325     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9    10  11  12  13\n",
       "  11464  69  2  108196  4   5  0   6  5  0  0  2993   0  40   0\n",
       "  14030  40  2  184378  5  10  1   5  1  0  0     0   0  60   7\n",
       "  20547  33  2  121195  1   9  0   9  0  4  0     0   0  50   0\n",
       "  12449  35  2  205338  1   9  1   6  1  0  0     0   0  40   0\n",
       "  7678   31  4  149184  0  13  1  12  1  0  0     0   0  97   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..   ...  ..  ..  ..\n",
       "  5893   32  2  211028  5  10  0   0  0  0  1     0   0  40   0\n",
       "  5652   56  5  188166  1   9  1  11  1  0  0     0   0  40   0\n",
       "  10516  46  2   24728  1   9  0   6  0  0  1     0   0  48   0\n",
       "  20181  60  4  232618  1   9  1   4  1  0  0     0   0  40   0\n",
       "  4842   60  2  225526  1   9  4   5  0  0  1     0   0  32   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9    10    11  12  13\n",
       "  30765  38  2  252897   5  10  2   5  3  0  1     0     0  25   0\n",
       "  31470  19  2  188568   5  10  0  14  0  0  1     0     0  40   0\n",
       "  11486  42  2  334522   1   9  1   2  1  1  0     0     0  40   0\n",
       "  5694   44  2  182616   7  11  1  10  1  0  0     0     0  40   0\n",
       "  16344  28  2   72443   5  10  0   1  0  0  0     0  1669  60   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..   ...   ...  ..  ..\n",
       "  11836  49  3  586657   0  13  1   1  1  0  0  7298     0  40   0\n",
       "  5712   47  0   55272   5  10  1   6  1  0  0     0     0  40   0\n",
       "  30750  25  2  122999   1   9  0   6  5  0  0     0     0  40   0\n",
       "  30147  31  2  243773   4   5  0   4  4  0  1     0     0  20   0\n",
       "  2278   17  2  329783  12   6  0   5  5  0  1     0     0  10   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9  10    11  12  13\n",
       "  21722  49  2  149809  14   1  1   4  1  0  0   0     0  40   4\n",
       "  22343  64  2  285052   5  10  6   1  0  0  1   0     0  10   0\n",
       "  1457   22  2  310152   5  10  0   7  0  0  0   0     0  40   0\n",
       "  18147  58  2  205410   1   9  1   6  1  0  0   0     0  40   0\n",
       "  19975  37  2  187748   5  10  1   6  1  0  0   0     0  40   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..  ..   ...  ..  ..\n",
       "  14954  35  2  409200   6  12  0  12  0  0  0   0     0  40   0\n",
       "  19342  26  2  266022   5  10  1   5  1  0  0   0     0  50   0\n",
       "  11229  32  2  244147   0  13  0   3  0  0  0   0  1876  50   0\n",
       "  31271  74  2   54732   5  10  1   5  1  0  0   0     0  20   0\n",
       "  31008  47  0  187087   0  13  1   1  1  0  0   0     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10  11  12  13\n",
       "  29537  46  4  133969  3  14  2   3  0  3  0      0   0  40   0\n",
       "  3593   61  1   32423  1   9  1   8  2  0  1  22040   0  40   0\n",
       "  2660   48  2  108993  1   9  1   6  1  0  0      0   0  45   0\n",
       "  32025  50  2  139703  5  10  1   6  1  0  0      0   0  40   0\n",
       "  4343   33  4   43959  3  14  1   3  1  0  0   7688   0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...  ..  ..  ..\n",
       "  20868  61  5  253101  0  13  2  11  0  0  1      0   0  24   0\n",
       "  10181  43  2  108945  5  10  0   1  0  0  1      0   0  50   0\n",
       "  18207  44  2  271792  0  13  3   1  4  0  1      0   0  40   0\n",
       "  20777  49  2  183013  1   9  1   8  1  0  0      0   0  40   0\n",
       "  30850  37  4  126569  1   9  0   6  0  0  0      0   0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10    11  12  13\n",
       "  6666   58  2   87329  2   7  1   0  1  0  0      0     0  48   0\n",
       "  6596   28  2  125527  5  10  0   5  0  0  0      0     0  50   0\n",
       "  29925  43  2  323713  1   9  1   9  1  0  0      0     0  40   0\n",
       "  18712  34  2   54850  0  13  0   1  0  0  0      0  1590  50   0\n",
       "  13786  53  2   47396  5  10  1   0  1  0  0      0     0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...   ...  ..  ..\n",
       "  28019  35  2  340428  6  12  0   0  0  0  1      0     0  35   0\n",
       "  17957  68  4  242095  0  13  1   1  1  0  0  20051     0  40   0\n",
       "  28106  22  2  221480  1   9  0   0  3  0  1      0     0  40   0\n",
       "  8949   60  5  366531  7  11  6  11  0  0  1      0     0  40   0\n",
       "  16147  41  2  152742  7  11  2  10  0  0  1   3325     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9    10  11  12  13\n",
       "  11464  69  2  108196  4   5  0   6  5  0  0  2993   0  40   0\n",
       "  14030  40  2  184378  5  10  1   5  1  0  0     0   0  60   7\n",
       "  20547  33  2  121195  1   9  0   9  0  4  0     0   0  50   0\n",
       "  12449  35  2  205338  1   9  1   6  1  0  0     0   0  40   0\n",
       "  7678   31  4  149184  0  13  1  12  1  0  0     0   0  97   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..   ...  ..  ..  ..\n",
       "  5893   32  2  211028  5  10  0   0  0  0  1     0   0  40   0\n",
       "  5652   56  5  188166  1   9  1  11  1  0  0     0   0  40   0\n",
       "  10516  46  2   24728  1   9  0   6  0  0  1     0   0  48   0\n",
       "  20181  60  4  232618  1   9  1   4  1  0  0     0   0  40   0\n",
       "  4842   60  2  225526  1   9  4   5  0  0  1     0   0  32   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9    10    11  12  13\n",
       "  30765  38  2  252897   5  10  2   5  3  0  1     0     0  25   0\n",
       "  31470  19  2  188568   5  10  0  14  0  0  1     0     0  40   0\n",
       "  11486  42  2  334522   1   9  1   2  1  1  0     0     0  40   0\n",
       "  5694   44  2  182616   7  11  1  10  1  0  0     0     0  40   0\n",
       "  16344  28  2   72443   5  10  0   1  0  0  0     0  1669  60   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..   ...   ...  ..  ..\n",
       "  11836  49  3  586657   0  13  1   1  1  0  0  7298     0  40   0\n",
       "  5712   47  0   55272   5  10  1   6  1  0  0     0     0  40   0\n",
       "  30750  25  2  122999   1   9  0   6  5  0  0     0     0  40   0\n",
       "  30147  31  2  243773   4   5  0   4  4  0  1     0     0  20   0\n",
       "  2278   17  2  329783  12   6  0   5  5  0  1     0     0  10   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9  10    11  12  13\n",
       "  21722  49  2  149809  14   1  1   4  1  0  0   0     0  40   4\n",
       "  22343  64  2  285052   5  10  6   1  0  0  1   0     0  10   0\n",
       "  1457   22  2  310152   5  10  0   7  0  0  0   0     0  40   0\n",
       "  18147  58  2  205410   1   9  1   6  1  0  0   0     0  40   0\n",
       "  19975  37  2  187748   5  10  1   6  1  0  0   0     0  40   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..  ..   ...  ..  ..\n",
       "  14954  35  2  409200   6  12  0  12  0  0  0   0     0  40   0\n",
       "  19342  26  2  266022   5  10  1   5  1  0  0   0     0  50   0\n",
       "  11229  32  2  244147   0  13  0   3  0  0  0   0  1876  50   0\n",
       "  31271  74  2   54732   5  10  1   5  1  0  0   0     0  20   0\n",
       "  31008  47  0  187087   0  13  1   1  1  0  0   0     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10  11  12  13\n",
       "  29537  46  4  133969  3  14  2   3  0  3  0      0   0  40   0\n",
       "  3593   61  1   32423  1   9  1   8  2  0  1  22040   0  40   0\n",
       "  2660   48  2  108993  1   9  1   6  1  0  0      0   0  45   0\n",
       "  32025  50  2  139703  5  10  1   6  1  0  0      0   0  40   0\n",
       "  4343   33  4   43959  3  14  1   3  1  0  0   7688   0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...  ..  ..  ..\n",
       "  20868  61  5  253101  0  13  2  11  0  0  1      0   0  24   0\n",
       "  10181  43  2  108945  5  10  0   1  0  0  1      0   0  50   0\n",
       "  18207  44  2  271792  0  13  3   1  4  0  1      0   0  40   0\n",
       "  20777  49  2  183013  1   9  1   8  1  0  0      0   0  40   0\n",
       "  30850  37  4  126569  1   9  0   6  0  0  0      0   0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10    11  12  13\n",
       "  6666   58  2   87329  2   7  1   0  1  0  0      0     0  48   0\n",
       "  6596   28  2  125527  5  10  0   5  0  0  0      0     0  50   0\n",
       "  29925  43  2  323713  1   9  1   9  1  0  0      0     0  40   0\n",
       "  18712  34  2   54850  0  13  0   1  0  0  0      0  1590  50   0\n",
       "  13786  53  2   47396  5  10  1   0  1  0  0      0     0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...   ...  ..  ..\n",
       "  28019  35  2  340428  6  12  0   0  0  0  1      0     0  35   0\n",
       "  17957  68  4  242095  0  13  1   1  1  0  0  20051     0  40   0\n",
       "  28106  22  2  221480  1   9  0   0  3  0  1      0     0  40   0\n",
       "  8949   60  5  366531  7  11  6  11  0  0  1      0     0  40   0\n",
       "  16147  41  2  152742  7  11  2  10  0  0  1   3325     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9    10  11  12  13\n",
       "  11464  69  2  108196  4   5  0   6  5  0  0  2993   0  40   0\n",
       "  14030  40  2  184378  5  10  1   5  1  0  0     0   0  60   7\n",
       "  20547  33  2  121195  1   9  0   9  0  4  0     0   0  50   0\n",
       "  12449  35  2  205338  1   9  1   6  1  0  0     0   0  40   0\n",
       "  7678   31  4  149184  0  13  1  12  1  0  0     0   0  97   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..   ...  ..  ..  ..\n",
       "  5893   32  2  211028  5  10  0   0  0  0  1     0   0  40   0\n",
       "  5652   56  5  188166  1   9  1  11  1  0  0     0   0  40   0\n",
       "  10516  46  2   24728  1   9  0   6  0  0  1     0   0  48   0\n",
       "  20181  60  4  232618  1   9  1   4  1  0  0     0   0  40   0\n",
       "  4842   60  2  225526  1   9  4   5  0  0  1     0   0  32   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9    10    11  12  13\n",
       "  30765  38  2  252897   5  10  2   5  3  0  1     0     0  25   0\n",
       "  31470  19  2  188568   5  10  0  14  0  0  1     0     0  40   0\n",
       "  11486  42  2  334522   1   9  1   2  1  1  0     0     0  40   0\n",
       "  5694   44  2  182616   7  11  1  10  1  0  0     0     0  40   0\n",
       "  16344  28  2   72443   5  10  0   1  0  0  0     0  1669  60   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..   ...   ...  ..  ..\n",
       "  11836  49  3  586657   0  13  1   1  1  0  0  7298     0  40   0\n",
       "  5712   47  0   55272   5  10  1   6  1  0  0     0     0  40   0\n",
       "  30750  25  2  122999   1   9  0   6  5  0  0     0     0  40   0\n",
       "  30147  31  2  243773   4   5  0   4  4  0  1     0     0  20   0\n",
       "  2278   17  2  329783  12   6  0   5  5  0  1     0     0  10   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9  10    11  12  13\n",
       "  21722  49  2  149809  14   1  1   4  1  0  0   0     0  40   4\n",
       "  22343  64  2  285052   5  10  6   1  0  0  1   0     0  10   0\n",
       "  1457   22  2  310152   5  10  0   7  0  0  0   0     0  40   0\n",
       "  18147  58  2  205410   1   9  1   6  1  0  0   0     0  40   0\n",
       "  19975  37  2  187748   5  10  1   6  1  0  0   0     0  40   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..  ..   ...  ..  ..\n",
       "  14954  35  2  409200   6  12  0  12  0  0  0   0     0  40   0\n",
       "  19342  26  2  266022   5  10  1   5  1  0  0   0     0  50   0\n",
       "  11229  32  2  244147   0  13  0   3  0  0  0   0  1876  50   0\n",
       "  31271  74  2   54732   5  10  1   5  1  0  0   0     0  20   0\n",
       "  31008  47  0  187087   0  13  1   1  1  0  0   0     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10  11  12  13\n",
       "  29537  46  4  133969  3  14  2   3  0  3  0      0   0  40   0\n",
       "  3593   61  1   32423  1   9  1   8  2  0  1  22040   0  40   0\n",
       "  2660   48  2  108993  1   9  1   6  1  0  0      0   0  45   0\n",
       "  32025  50  2  139703  5  10  1   6  1  0  0      0   0  40   0\n",
       "  4343   33  4   43959  3  14  1   3  1  0  0   7688   0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...  ..  ..  ..\n",
       "  20868  61  5  253101  0  13  2  11  0  0  1      0   0  24   0\n",
       "  10181  43  2  108945  5  10  0   1  0  0  1      0   0  50   0\n",
       "  18207  44  2  271792  0  13  3   1  4  0  1      0   0  40   0\n",
       "  20777  49  2  183013  1   9  1   8  1  0  0      0   0  40   0\n",
       "  30850  37  4  126569  1   9  0   6  0  0  0      0   0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10    11  12  13\n",
       "  6666   58  2   87329  2   7  1   0  1  0  0      0     0  48   0\n",
       "  6596   28  2  125527  5  10  0   5  0  0  0      0     0  50   0\n",
       "  29925  43  2  323713  1   9  1   9  1  0  0      0     0  40   0\n",
       "  18712  34  2   54850  0  13  0   1  0  0  0      0  1590  50   0\n",
       "  13786  53  2   47396  5  10  1   0  1  0  0      0     0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...   ...  ..  ..\n",
       "  28019  35  2  340428  6  12  0   0  0  0  1      0     0  35   0\n",
       "  17957  68  4  242095  0  13  1   1  1  0  0  20051     0  40   0\n",
       "  28106  22  2  221480  1   9  0   0  3  0  1      0     0  40   0\n",
       "  8949   60  5  366531  7  11  6  11  0  0  1      0     0  40   0\n",
       "  16147  41  2  152742  7  11  2  10  0  0  1   3325     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9    10  11  12  13\n",
       "  11464  69  2  108196  4   5  0   6  5  0  0  2993   0  40   0\n",
       "  14030  40  2  184378  5  10  1   5  1  0  0     0   0  60   7\n",
       "  20547  33  2  121195  1   9  0   9  0  4  0     0   0  50   0\n",
       "  12449  35  2  205338  1   9  1   6  1  0  0     0   0  40   0\n",
       "  7678   31  4  149184  0  13  1  12  1  0  0     0   0  97   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..   ...  ..  ..  ..\n",
       "  5893   32  2  211028  5  10  0   0  0  0  1     0   0  40   0\n",
       "  5652   56  5  188166  1   9  1  11  1  0  0     0   0  40   0\n",
       "  10516  46  2   24728  1   9  0   6  0  0  1     0   0  48   0\n",
       "  20181  60  4  232618  1   9  1   4  1  0  0     0   0  40   0\n",
       "  4842   60  2  225526  1   9  4   5  0  0  1     0   0  32   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9    10    11  12  13\n",
       "  30765  38  2  252897   5  10  2   5  3  0  1     0     0  25   0\n",
       "  31470  19  2  188568   5  10  0  14  0  0  1     0     0  40   0\n",
       "  11486  42  2  334522   1   9  1   2  1  1  0     0     0  40   0\n",
       "  5694   44  2  182616   7  11  1  10  1  0  0     0     0  40   0\n",
       "  16344  28  2   72443   5  10  0   1  0  0  0     0  1669  60   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..   ...   ...  ..  ..\n",
       "  11836  49  3  586657   0  13  1   1  1  0  0  7298     0  40   0\n",
       "  5712   47  0   55272   5  10  1   6  1  0  0     0     0  40   0\n",
       "  30750  25  2  122999   1   9  0   6  5  0  0     0     0  40   0\n",
       "  30147  31  2  243773   4   5  0   4  4  0  1     0     0  20   0\n",
       "  2278   17  2  329783  12   6  0   5  5  0  1     0     0  10   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9  10    11  12  13\n",
       "  21722  49  2  149809  14   1  1   4  1  0  0   0     0  40   4\n",
       "  22343  64  2  285052   5  10  6   1  0  0  1   0     0  10   0\n",
       "  1457   22  2  310152   5  10  0   7  0  0  0   0     0  40   0\n",
       "  18147  58  2  205410   1   9  1   6  1  0  0   0     0  40   0\n",
       "  19975  37  2  187748   5  10  1   6  1  0  0   0     0  40   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..  ..   ...  ..  ..\n",
       "  14954  35  2  409200   6  12  0  12  0  0  0   0     0  40   0\n",
       "  19342  26  2  266022   5  10  1   5  1  0  0   0     0  50   0\n",
       "  11229  32  2  244147   0  13  0   3  0  0  0   0  1876  50   0\n",
       "  31271  74  2   54732   5  10  1   5  1  0  0   0     0  20   0\n",
       "  31008  47  0  187087   0  13  1   1  1  0  0   0     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10  11  12  13\n",
       "  29537  46  4  133969  3  14  2   3  0  3  0      0   0  40   0\n",
       "  3593   61  1   32423  1   9  1   8  2  0  1  22040   0  40   0\n",
       "  2660   48  2  108993  1   9  1   6  1  0  0      0   0  45   0\n",
       "  32025  50  2  139703  5  10  1   6  1  0  0      0   0  40   0\n",
       "  4343   33  4   43959  3  14  1   3  1  0  0   7688   0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...  ..  ..  ..\n",
       "  20868  61  5  253101  0  13  2  11  0  0  1      0   0  24   0\n",
       "  10181  43  2  108945  5  10  0   1  0  0  1      0   0  50   0\n",
       "  18207  44  2  271792  0  13  3   1  4  0  1      0   0  40   0\n",
       "  20777  49  2  183013  1   9  1   8  1  0  0      0   0  40   0\n",
       "  30850  37  4  126569  1   9  0   6  0  0  0      0   0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10    11  12  13\n",
       "  6666   58  2   87329  2   7  1   0  1  0  0      0     0  48   0\n",
       "  6596   28  2  125527  5  10  0   5  0  0  0      0     0  50   0\n",
       "  29925  43  2  323713  1   9  1   9  1  0  0      0     0  40   0\n",
       "  18712  34  2   54850  0  13  0   1  0  0  0      0  1590  50   0\n",
       "  13786  53  2   47396  5  10  1   0  1  0  0      0     0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...   ...  ..  ..\n",
       "  28019  35  2  340428  6  12  0   0  0  0  1      0     0  35   0\n",
       "  17957  68  4  242095  0  13  1   1  1  0  0  20051     0  40   0\n",
       "  28106  22  2  221480  1   9  0   0  3  0  1      0     0  40   0\n",
       "  8949   60  5  366531  7  11  6  11  0  0  1      0     0  40   0\n",
       "  16147  41  2  152742  7  11  2  10  0  0  1   3325     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9    10  11  12  13\n",
       "  11464  69  2  108196  4   5  0   6  5  0  0  2993   0  40   0\n",
       "  14030  40  2  184378  5  10  1   5  1  0  0     0   0  60   7\n",
       "  20547  33  2  121195  1   9  0   9  0  4  0     0   0  50   0\n",
       "  12449  35  2  205338  1   9  1   6  1  0  0     0   0  40   0\n",
       "  7678   31  4  149184  0  13  1  12  1  0  0     0   0  97   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..   ...  ..  ..  ..\n",
       "  5893   32  2  211028  5  10  0   0  0  0  1     0   0  40   0\n",
       "  5652   56  5  188166  1   9  1  11  1  0  0     0   0  40   0\n",
       "  10516  46  2   24728  1   9  0   6  0  0  1     0   0  48   0\n",
       "  20181  60  4  232618  1   9  1   4  1  0  0     0   0  40   0\n",
       "  4842   60  2  225526  1   9  4   5  0  0  1     0   0  32   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9    10    11  12  13\n",
       "  30765  38  2  252897   5  10  2   5  3  0  1     0     0  25   0\n",
       "  31470  19  2  188568   5  10  0  14  0  0  1     0     0  40   0\n",
       "  11486  42  2  334522   1   9  1   2  1  1  0     0     0  40   0\n",
       "  5694   44  2  182616   7  11  1  10  1  0  0     0     0  40   0\n",
       "  16344  28  2   72443   5  10  0   1  0  0  0     0  1669  60   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..   ...   ...  ..  ..\n",
       "  11836  49  3  586657   0  13  1   1  1  0  0  7298     0  40   0\n",
       "  5712   47  0   55272   5  10  1   6  1  0  0     0     0  40   0\n",
       "  30750  25  2  122999   1   9  0   6  5  0  0     0     0  40   0\n",
       "  30147  31  2  243773   4   5  0   4  4  0  1     0     0  20   0\n",
       "  2278   17  2  329783  12   6  0   5  5  0  1     0     0  10   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2   3   4  5   6  7  8  9  10    11  12  13\n",
       "  21722  49  2  149809  14   1  1   4  1  0  0   0     0  40   4\n",
       "  22343  64  2  285052   5  10  6   1  0  0  1   0     0  10   0\n",
       "  1457   22  2  310152   5  10  0   7  0  0  0   0     0  40   0\n",
       "  18147  58  2  205410   1   9  1   6  1  0  0   0     0  40   0\n",
       "  19975  37  2  187748   5  10  1   6  1  0  0   0     0  40   0\n",
       "  ...    .. ..     ...  ..  .. ..  .. .. .. ..  ..   ...  ..  ..\n",
       "  14954  35  2  409200   6  12  0  12  0  0  0   0     0  40   0\n",
       "  19342  26  2  266022   5  10  1   5  1  0  0   0     0  50   0\n",
       "  11229  32  2  244147   0  13  0   3  0  0  0   0  1876  50   0\n",
       "  31271  74  2   54732   5  10  1   5  1  0  0   0     0  20   0\n",
       "  31008  47  0  187087   0  13  1   1  1  0  0   0     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10  11  12  13\n",
       "  29537  46  4  133969  3  14  2   3  0  3  0      0   0  40   0\n",
       "  3593   61  1   32423  1   9  1   8  2  0  1  22040   0  40   0\n",
       "  2660   48  2  108993  1   9  1   6  1  0  0      0   0  45   0\n",
       "  32025  50  2  139703  5  10  1   6  1  0  0      0   0  40   0\n",
       "  4343   33  4   43959  3  14  1   3  1  0  0   7688   0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...  ..  ..  ..\n",
       "  20868  61  5  253101  0  13  2  11  0  0  1      0   0  24   0\n",
       "  10181  43  2  108945  5  10  0   1  0  0  1      0   0  50   0\n",
       "  18207  44  2  271792  0  13  3   1  4  0  1      0   0  40   0\n",
       "  20777  49  2  183013  1   9  1   8  1  0  0      0   0  40   0\n",
       "  30850  37  4  126569  1   9  0   6  0  0  0      0   0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9     10    11  12  13\n",
       "  6666   58  2   87329  2   7  1   0  1  0  0      0     0  48   0\n",
       "  6596   28  2  125527  5  10  0   5  0  0  0      0     0  50   0\n",
       "  29925  43  2  323713  1   9  1   9  1  0  0      0     0  40   0\n",
       "  18712  34  2   54850  0  13  0   1  0  0  0      0  1590  50   0\n",
       "  13786  53  2   47396  5  10  1   0  1  0  0      0     0  50   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..    ...   ...  ..  ..\n",
       "  28019  35  2  340428  6  12  0   0  0  0  1      0     0  35   0\n",
       "  17957  68  4  242095  0  13  1   1  1  0  0  20051     0  40   0\n",
       "  28106  22  2  221480  1   9  0   0  3  0  1      0     0  40   0\n",
       "  8949   60  5  366531  7  11  6  11  0  0  1      0     0  40   0\n",
       "  16147  41  2  152742  7  11  2  10  0  0  1   3325     0  40   0\n",
       "  \n",
       "  [19536 rows x 14 columns],\n",
       "          0  1       2  3   4  5   6  7  8  9    10  11  12  13\n",
       "  11464  69  2  108196  4   5  0   6  5  0  0  2993   0  40   0\n",
       "  14030  40  2  184378  5  10  1   5  1  0  0     0   0  60   7\n",
       "  20547  33  2  121195  1   9  0   9  0  4  0     0   0  50   0\n",
       "  12449  35  2  205338  1   9  1   6  1  0  0     0   0  40   0\n",
       "  7678   31  4  149184  0  13  1  12  1  0  0     0   0  97   0\n",
       "  ...    .. ..     ... ..  .. ..  .. .. .. ..   ...  ..  ..  ..\n",
       "  5893   32  2  211028  5  10  0   0  0  0  1     0   0  40   0\n",
       "  5652   56  5  188166  1   9  1  11  1  0  0     0   0  40   0\n",
       "  10516  46  2   24728  1   9  0   6  0  0  1     0   0  48   0\n",
       "  20181  60  4  232618  1   9  1   4  1  0  0     0   0  40   0\n",
       "  4842   60  2  225526  1   9  4   5  0  0  1     0   0  32   0\n",
       "  \n",
       "  [19536 rows x 14 columns]],\n",
       " [       14\n",
       "  30765   0\n",
       "  31470   0\n",
       "  11486   0\n",
       "  5694    1\n",
       "  16344   0\n",
       "  ...    ..\n",
       "  11836   1\n",
       "  5712    1\n",
       "  30750   0\n",
       "  30147   0\n",
       "  2278    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  21722   0\n",
       "  22343   0\n",
       "  1457    0\n",
       "  18147   0\n",
       "  19975   0\n",
       "  ...    ..\n",
       "  14954   0\n",
       "  19342   0\n",
       "  11229   0\n",
       "  31271   1\n",
       "  31008   1\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  29537   0\n",
       "  3593    0\n",
       "  2660    1\n",
       "  32025   1\n",
       "  4343    1\n",
       "  ...    ..\n",
       "  20868   0\n",
       "  10181   0\n",
       "  18207   0\n",
       "  20777   0\n",
       "  30850   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  6666    0\n",
       "  6596    0\n",
       "  29925   0\n",
       "  18712   0\n",
       "  13786   1\n",
       "  ...    ..\n",
       "  28019   0\n",
       "  17957   1\n",
       "  28106   0\n",
       "  8949    0\n",
       "  16147   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  11464   0\n",
       "  14030   0\n",
       "  20547   0\n",
       "  12449   0\n",
       "  7678    1\n",
       "  ...    ..\n",
       "  5893    0\n",
       "  5652    0\n",
       "  10516   0\n",
       "  20181   0\n",
       "  4842    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  30765   0\n",
       "  31470   0\n",
       "  11486   0\n",
       "  5694    1\n",
       "  16344   0\n",
       "  ...    ..\n",
       "  11836   1\n",
       "  5712    1\n",
       "  30750   0\n",
       "  30147   0\n",
       "  2278    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  21722   0\n",
       "  22343   0\n",
       "  1457    0\n",
       "  18147   0\n",
       "  19975   0\n",
       "  ...    ..\n",
       "  14954   0\n",
       "  19342   0\n",
       "  11229   0\n",
       "  31271   1\n",
       "  31008   1\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  29537   0\n",
       "  3593    0\n",
       "  2660    1\n",
       "  32025   1\n",
       "  4343    1\n",
       "  ...    ..\n",
       "  20868   0\n",
       "  10181   0\n",
       "  18207   0\n",
       "  20777   0\n",
       "  30850   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  6666    0\n",
       "  6596    0\n",
       "  29925   0\n",
       "  18712   0\n",
       "  13786   1\n",
       "  ...    ..\n",
       "  28019   0\n",
       "  17957   1\n",
       "  28106   0\n",
       "  8949    0\n",
       "  16147   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  11464   0\n",
       "  14030   0\n",
       "  20547   0\n",
       "  12449   0\n",
       "  7678    1\n",
       "  ...    ..\n",
       "  5893    0\n",
       "  5652    0\n",
       "  10516   0\n",
       "  20181   0\n",
       "  4842    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  30765   0\n",
       "  31470   0\n",
       "  11486   0\n",
       "  5694    1\n",
       "  16344   0\n",
       "  ...    ..\n",
       "  11836   1\n",
       "  5712    1\n",
       "  30750   0\n",
       "  30147   0\n",
       "  2278    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  21722   0\n",
       "  22343   0\n",
       "  1457    0\n",
       "  18147   0\n",
       "  19975   0\n",
       "  ...    ..\n",
       "  14954   0\n",
       "  19342   0\n",
       "  11229   0\n",
       "  31271   1\n",
       "  31008   1\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  29537   0\n",
       "  3593    0\n",
       "  2660    1\n",
       "  32025   1\n",
       "  4343    1\n",
       "  ...    ..\n",
       "  20868   0\n",
       "  10181   0\n",
       "  18207   0\n",
       "  20777   0\n",
       "  30850   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  6666    0\n",
       "  6596    0\n",
       "  29925   0\n",
       "  18712   0\n",
       "  13786   1\n",
       "  ...    ..\n",
       "  28019   0\n",
       "  17957   1\n",
       "  28106   0\n",
       "  8949    0\n",
       "  16147   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  11464   0\n",
       "  14030   0\n",
       "  20547   0\n",
       "  12449   0\n",
       "  7678    1\n",
       "  ...    ..\n",
       "  5893    0\n",
       "  5652    0\n",
       "  10516   0\n",
       "  20181   0\n",
       "  4842    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  30765   0\n",
       "  31470   0\n",
       "  11486   0\n",
       "  5694    1\n",
       "  16344   0\n",
       "  ...    ..\n",
       "  11836   1\n",
       "  5712    1\n",
       "  30750   0\n",
       "  30147   0\n",
       "  2278    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  21722   0\n",
       "  22343   0\n",
       "  1457    0\n",
       "  18147   0\n",
       "  19975   0\n",
       "  ...    ..\n",
       "  14954   0\n",
       "  19342   0\n",
       "  11229   0\n",
       "  31271   1\n",
       "  31008   1\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  29537   0\n",
       "  3593    0\n",
       "  2660    1\n",
       "  32025   1\n",
       "  4343    1\n",
       "  ...    ..\n",
       "  20868   0\n",
       "  10181   0\n",
       "  18207   0\n",
       "  20777   0\n",
       "  30850   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  6666    0\n",
       "  6596    0\n",
       "  29925   0\n",
       "  18712   0\n",
       "  13786   1\n",
       "  ...    ..\n",
       "  28019   0\n",
       "  17957   1\n",
       "  28106   0\n",
       "  8949    0\n",
       "  16147   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  11464   0\n",
       "  14030   0\n",
       "  20547   0\n",
       "  12449   0\n",
       "  7678    1\n",
       "  ...    ..\n",
       "  5893    0\n",
       "  5652    0\n",
       "  10516   0\n",
       "  20181   0\n",
       "  4842    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  30765   0\n",
       "  31470   0\n",
       "  11486   0\n",
       "  5694    1\n",
       "  16344   0\n",
       "  ...    ..\n",
       "  11836   1\n",
       "  5712    1\n",
       "  30750   0\n",
       "  30147   0\n",
       "  2278    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  21722   0\n",
       "  22343   0\n",
       "  1457    0\n",
       "  18147   0\n",
       "  19975   0\n",
       "  ...    ..\n",
       "  14954   0\n",
       "  19342   0\n",
       "  11229   0\n",
       "  31271   1\n",
       "  31008   1\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  29537   0\n",
       "  3593    0\n",
       "  2660    1\n",
       "  32025   1\n",
       "  4343    1\n",
       "  ...    ..\n",
       "  20868   0\n",
       "  10181   0\n",
       "  18207   0\n",
       "  20777   0\n",
       "  30850   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  6666    0\n",
       "  6596    0\n",
       "  29925   0\n",
       "  18712   0\n",
       "  13786   1\n",
       "  ...    ..\n",
       "  28019   0\n",
       "  17957   1\n",
       "  28106   0\n",
       "  8949    0\n",
       "  16147   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  11464   0\n",
       "  14030   0\n",
       "  20547   0\n",
       "  12449   0\n",
       "  7678    1\n",
       "  ...    ..\n",
       "  5893    0\n",
       "  5652    0\n",
       "  10516   0\n",
       "  20181   0\n",
       "  4842    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  30765   0\n",
       "  31470   0\n",
       "  11486   0\n",
       "  5694    1\n",
       "  16344   0\n",
       "  ...    ..\n",
       "  11836   1\n",
       "  5712    1\n",
       "  30750   0\n",
       "  30147   0\n",
       "  2278    0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  21722   0\n",
       "  22343   0\n",
       "  1457    0\n",
       "  18147   0\n",
       "  19975   0\n",
       "  ...    ..\n",
       "  14954   0\n",
       "  19342   0\n",
       "  11229   0\n",
       "  31271   1\n",
       "  31008   1\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  29537   0\n",
       "  3593    0\n",
       "  2660    1\n",
       "  32025   1\n",
       "  4343    1\n",
       "  ...    ..\n",
       "  20868   0\n",
       "  10181   0\n",
       "  18207   0\n",
       "  20777   0\n",
       "  30850   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  6666    0\n",
       "  6596    0\n",
       "  29925   0\n",
       "  18712   0\n",
       "  13786   1\n",
       "  ...    ..\n",
       "  28019   0\n",
       "  17957   1\n",
       "  28106   0\n",
       "  8949    0\n",
       "  16147   0\n",
       "  \n",
       "  [19536 rows x 1 columns],        14\n",
       "  11464   0\n",
       "  14030   0\n",
       "  20547   0\n",
       "  12449   0\n",
       "  7678    1\n",
       "  ...    ..\n",
       "  5893    0\n",
       "  5652    0\n",
       "  10516   0\n",
       "  20181   0\n",
       "  4842    0\n",
       "  \n",
       "  [19536 rows x 1 columns]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=bootstrap(boot)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 작성한 두 bootstrap 방법을 사용하여 실습 1에서 만든 모델들을 학습시키고, 적절하게 aggregating하여 결과를 도출해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[('GNBC', GaussianNB())]+[(f'kNN-{n}', KNeighborsClassifier(n)) for n in [2]]+\\\n",
    "        [(f'DT-{_}',DecisionTreeClassifier()) for _ in range(3)]\n",
    "\n",
    "copy_models = models.copy()\n",
    "\n",
    "ensemble = VotingClassifier(models, voting='hard')\n",
    "name=[]\n",
    "score=[]\n",
    "for i in range(len(boot)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data[0][i], data[1][i], test_size=0.3, shuffle=False)\n",
    "    ensemble.fit(x_train.values, y_train.values.ravel())\n",
    "    for clf in copy_models:\n",
    "        name.append(clf[0])\n",
    "        clf[1].fit(x_train.values,y_train.values.ravel())\n",
    "        score.append(clf[1].score(x_test,y_test)*100)\n",
    "#         print(f\"{clf[0]}'s score : {clf[1].score(x_test,y_test)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_score=[0,0,0,0,0]\n",
    "result_score[0] = score[0]+score[5]+score[10]+score[15]+score[20]\n",
    "result_score[1] = score[1]+score[6]+score[11]+score[16]+score[21]\n",
    "result_score[2] = score[2]+score[7]+score[12]+score[17]+score[22]\n",
    "result_score[3] = score[3]+score[8]+score[13]+score[18]+score[23]\n",
    "result_score[4] = score[4]+score[9]+score[14]+score[19]+score[24]\n",
    "for i in range(0,5):\n",
    "    result_score[i]=result_score[i]/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 모델에 셈플링되어 나누어진 5개의 학습 데이터를 각각 넣어보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNBC'score : 79.70312233407269\n",
      "kNN-2'score : 80.89063299778195\n",
      "DT-0'score : 87.44924074390036\n",
      "DT-1'score : 87.40829210032419\n",
      "DT-2'score : 87.4970141614059\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(name[i]+\"'score :\", result_score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 지난 실슴들을 통해 다양한 preprocessing을 실습하였습니다\n",
    "\n",
    "앞서 생성한 모델을 학습시킬때 성능을 개선시킬 방법이 있는지 고민해보시고 방법론을 아래 markdown으로 작성해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난 실습들을 통해 다양한 preprocessing을 이용하여 학습의 성능을 높혀왔다. bagging도 분산이나 noise, bias를 잘 처리하면 더욱 성능이 높아질것으로 예상한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 03. Random forest & gradient boosting machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest와 gradient boosting machine에 대해 사용해보시고 결과를 출력해주세요\n",
    "\n",
    "hyper parameter는 자유롭게 사용해주시면 됩니다\n",
    "\n",
    "그리고 feature importance 구하여 각 base,weak learner들이 어떻게 instance를 분리하였는지 간단히 설명해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble score : 89.609%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train.values, y_train.values.ravel())\n",
    "\n",
    "print (f'ensemble score : {rf.score(x_test,y_test)*100:.5}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0570           26.60m\n",
      "         2           1.0092           24.96m\n",
      "         3           0.9717           23.29m\n",
      "         4           0.9407           22.86m\n",
      "         5           0.9154           22.62m\n",
      "         6           0.8936           22.44m\n",
      "         7           0.8709           22.33m\n",
      "         8           0.8491           22.03m\n",
      "         9           0.8270           21.98m\n",
      "        10           0.8145           21.94m\n",
      "        20           0.7134           21.02m\n",
      "        30           0.6632           20.72m\n",
      "        40           0.6335           20.82m\n",
      "        50           0.6152           20.67m\n",
      "        60           0.6014           20.60m\n",
      "        70           0.5889           20.53m\n",
      "        80           0.5798           20.47m\n",
      "        90           0.5717           20.61m\n",
      "       100           0.5646           20.65m\n",
      "       200           0.5125           20.49m\n",
      "       300           0.4829           20.53m\n",
      "       400           0.4587           20.39m\n",
      "       500           0.4385           20.22m\n",
      "       600           0.4206           20.10m\n",
      "       700           0.4021           20.02m\n",
      "       800           0.3863           19.94m\n",
      "       900           0.3707           19.89m\n",
      "      1000           0.3565           19.84m\n",
      "      2000           0.2536           19.54m\n",
      "학습하는 동안 생성한 weak learner의 갯수 :  2145\n",
      "모델의 정확도 : 88.739%\n"
     ]
    }
   ],
   "source": [
    "gbc=GradientBoostingClassifier(n_estimators=100000,n_iter_no_change=1000,verbose=1)\n",
    "gbc.fit(x_train.values,y_train.values.ravel())\n",
    "\n",
    "print('학습하는 동안 생성한 weak learner의 갯수 : ', gbc.n_estimators_)\n",
    "print(f'모델의 정확도 : {gbc.score(x_test.values,y_test.values.ravel())*100:.5}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 04. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 다양한 base learner를 사용하여 ensemble을 시도하였습니다\n",
    "\n",
    "그리고 Ensemble의 결과를 얻기 위해 majority voting, argmax $p(\\hat{y})$ 등을 시도하였습니다\n",
    "\n",
    "다른 측면에서 voting 자체도 기계학습으로 대체하는 기법이 있습니다\n",
    "\n",
    "이것을 stacking이라고 부릅니다\n",
    "\n",
    "간단한 모델들을 사용하여 base learner들의 예측 확률 값을 최종 모델의 input으로 사용하는 stacking model을 생성해주세요\n",
    "\n",
    "Stacking model은 기존과 같은 target을 학습하며, 이렇게 학습된 모델이 어떤 예측 성능을 보이는지 보여주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 수업은 너무 어려워서 이해하기가 힘듭니다..\n",
    "최선을 다했지만 이것이 한계인것같습니다 ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
